最近看《写给程序员的数据挖掘指南》，研究推荐算法，书中的测试数据集是 Book-Crossing Dataset 提供的亚马逊用户对书籍评分的真实数据。推荐大家看本书，写得不错，立刻就能对推荐算法上手，甚至应用到你的项目中。

Book-Crossing Dataset 提供两种格式的数据集：CVS 格式和 SQL dump，问题是：

如果你有 UE 打开 cvs 文件，有乱码。无论如何转换编码，都不行~因为，这个文件是亚马逊通过程序持久化后，再导出来的。你还会发现，文件中有 html 标记，另外，关于用户名，书名等等信息，基本都是德文的（看域名就知道了）~

虽然，作者提供了加载测试数据集的 python 代码，不过不能导入到 MySQL 数据库中，其中，作者只是简单地按分号来分割字段内容（虽然推荐算法并不需要全部字段），可数据集中包含类似“&#2345;”或“\“”这样的字符，不可能导入到 MySQL 数据库中~

你也许会问，作者都不导入到数据库，你为什么要导？因为，作者提供的推荐算法属于内存模型，也就是一次性把数据加载到内存，但之前，总还是要持久化吧~

因此，只能改造一下作者的 Python 代码~

word2vec介绍
word2vec官网：https://code.google.com/p/word2vec/

word2vec是google的一个开源工具，能够根据输入的词的集合计算出词与词之间的距离。
它将term转换成向量形式，可以把对文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。
word2vec计算的是余弦值，距离范围为0-1之间，值越大代表两个词关联度越高。
词向量：用Distributed Representation表示词，通常也被称为“Word Representation”或“Word Embedding（嵌入）”。
简言之：词向量表示法让相关或者相似的词，在距离上更接近。

具体使用（处理中文）
收集语料
本文：亚马逊中文书评语料，12万+句子文本。 
语料以纯文本形式存入txt文本。 
注意： 
理论上语料越大越好 
理论上语料越大越好 
理论上语料越大越好 
重要的事情说三遍。 
因为太小的语料跑出来的结果并没有太大意义。

分词
中文分词工具还是很多的，我自己常用的： 
- 中科院NLPIR 
- 哈工大LTP 
--------------------- 
作者：竹聿Simon 
来源：CSDN 
原文：https://blog.csdn.net/churximi/article/details/51472300 
版权声明：本文为博主原创文章，转载请附上博文链接！